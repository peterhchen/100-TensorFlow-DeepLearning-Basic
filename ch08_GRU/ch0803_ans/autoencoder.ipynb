{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "autoencoder.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "3NUbHfInxbLp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        },
        "outputId": "cd135d84-8492-42ee-8bc5-59de85b0dbce"
      },
      "cell_type": "code",
      "source": [
        "\"\"\" Auto Encoder Example.\n",
        "Build a 2 layers auto-encoder with TensorFlow to compress images to a\n",
        "lower latent space and then reconstruct them.\n",
        "References:\n",
        "    Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. \"Gradient-based\n",
        "    learning applied to document recognition.\" Proceedings of the IEEE,\n",
        "    86(11):2278-2324, November 1998.\n",
        "Links: [MNIST Dataset] http://yann.lecun.com/exdb/mnist/\n",
        "Author: Aymeric Damien\n",
        "Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import division, print_function, absolute_import\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import MNIST data\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
        "\n",
        "# Training Parameters\n",
        "learning_rate = 0.01\n",
        "num_steps = 30000\n",
        "batch_size = 256\n",
        "\n",
        "display_step = 1000\n",
        "examples_to_show = 10\n",
        "\n",
        "# Network Parameters\n",
        "num_hidden_1 = 256 # 1st layer num features\n",
        "num_hidden_2 = 128 # 2nd layer num features (the latent dim)\n",
        "num_input = 784 # MNIST data input (img shape: 28*28)\n",
        "\n",
        "# tf Graph input (only pictures)\n",
        "X = tf.placeholder(\"float\", [None, num_input])\n",
        "\n",
        "weights = {\n",
        "    'encoder_h1': tf.Variable(tf.random_normal([num_input, num_hidden_1])),\n",
        "    'encoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_hidden_2])),\n",
        "    'decoder_h1': tf.Variable(tf.random_normal([num_hidden_2, num_hidden_1])),\n",
        "    'decoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_input])),\n",
        "}\n",
        "biases = {\n",
        "    'encoder_b1': tf.Variable(tf.random_normal([num_hidden_1])),\n",
        "    'encoder_b2': tf.Variable(tf.random_normal([num_hidden_2])),\n",
        "    'decoder_b1': tf.Variable(tf.random_normal([num_hidden_1])),\n",
        "    'decoder_b2': tf.Variable(tf.random_normal([num_input])),\n",
        "}\n",
        "\n",
        "# Building the encoder\n",
        "def encoder(x):\n",
        "    # Encoder Hidden layer with sigmoid activation #1\n",
        "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']),\n",
        "                                   biases['encoder_b1']))\n",
        "    # Encoder Hidden layer with sigmoid activation #2\n",
        "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']),\n",
        "                                   biases['encoder_b2']))\n",
        "    return layer_2\n",
        "\n",
        "\n",
        "# Building the decoder\n",
        "def decoder(x):\n",
        "    # Decoder Hidden layer with sigmoid activation #1\n",
        "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']),\n",
        "                                   biases['decoder_b1']))\n",
        "    # Decoder Hidden layer with sigmoid activation #2\n",
        "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']),\n",
        "                                   biases['decoder_b2']))\n",
        "    return layer_2\n",
        "\n",
        "# Construct model\n",
        "encoder_op = encoder(X)\n",
        "decoder_op = decoder(encoder_op)\n",
        "\n",
        "# Prediction\n",
        "y_pred = decoder_op\n",
        "# Targets (Labels) are the input data.\n",
        "y_true = X\n",
        "\n",
        "# Define loss and optimizer, minimize the squared error\n",
        "loss = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
        "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
        "\n",
        "# Initialize the variables (i.e. assign their default value)\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Training: Start a new TF session\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "\n",
        "    for i in range(1, num_steps+1):\n",
        "        # Prepare Data\n",
        "        # Get the next batch of MNIST data (only images are needed, not labels)\n",
        "        batch_x, _ = mnist.train.next_batch(batch_size)\n",
        "\n",
        "        # Run optimization op (backprop) and cost op (to get loss value)\n",
        "        _, l = sess.run([optimizer, loss], feed_dict={X: batch_x})\n",
        "        # Display logs per step\n",
        "        if i % display_step == 0 or i == 1:\n",
        "            print('Step %i: Minibatch Loss: %f' % (i, l))\n",
        "\n",
        "    # Testing: Encode/decode images from test set\n",
        "    n = 4\n",
        "    canvas_orig = np.empty((28 * n, 28 * n))\n",
        "    canvas_recon = np.empty((28 * n, 28 * n))\n",
        "\n",
        "    for i in range(n):\n",
        "        # MNIST test set\n",
        "        batch_x, _ = mnist.test.next_batch(n)\n",
        "        # Encode and decode the digit image\n",
        "        g = sess.run(decoder_op, feed_dict={X: batch_x})\n",
        "\n",
        "        # Display original images\n",
        "        for j in range(n):\n",
        "            # Draw the original digits\n",
        "            canvas_orig[i * 28:(i + 1) * 28, j * 28:(j + 1) * 28] = \\\n",
        "                batch_x[j].reshape([28, 28])\n",
        "        # Display reconstructed images\n",
        "        for j in range(n):\n",
        "            # Draw the reconstructed digits\n",
        "            canvas_recon[i * 28:(i + 1) * 28, j * 28:(j + 1) * 28] = \\\n",
        "                g[j].reshape([28, 28])\n",
        "\n",
        "    print(\"Original Images\")\n",
        "    plt.figure(figsize=(n, n))\n",
        "    plt.imshow(canvas_orig, origin=\"upper\", cmap=\"gray\")\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Reconstructed Images\")\n",
        "    plt.figure(figsize=(n, n))\n",
        "    plt.imshow(canvas_recon, origin=\"upper\", cmap=\"gray\")\n",
        "    plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-1-5c82bb47684d>:21: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "Step 1: Minibatch Loss: 0.447914\n",
            "Step 1000: Minibatch Loss: 0.144458\n",
            "Step 2000: Minibatch Loss: 0.126958\n",
            "Step 3000: Minibatch Loss: 0.116875\n",
            "Step 4000: Minibatch Loss: 0.111812\n",
            "Step 5000: Minibatch Loss: 0.105051\n",
            "Step 6000: Minibatch Loss: 0.100564\n",
            "Step 7000: Minibatch Loss: 0.095159\n",
            "Step 8000: Minibatch Loss: 0.092344\n",
            "Step 9000: Minibatch Loss: 0.087017\n",
            "Step 10000: Minibatch Loss: 0.087437\n",
            "Step 11000: Minibatch Loss: 0.085134\n",
            "Step 12000: Minibatch Loss: 0.082185\n",
            "Step 13000: Minibatch Loss: 0.080886\n",
            "Step 14000: Minibatch Loss: 0.078143\n",
            "Step 15000: Minibatch Loss: 0.078368\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}