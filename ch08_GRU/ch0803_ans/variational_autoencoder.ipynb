{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "variational-autoencoder.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "ZVdvvdWl0MeK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "9ab4af3c-efd9-427f-dce2-69955c812622"
      },
      "cell_type": "code",
      "source": [
        "\"\"\" Variational Auto-Encoder Example.\n",
        "Using a variational auto-encoder to generate digits images from noise.\n",
        "MNIST handwritten digits are used as training examples.\n",
        "References:\n",
        "- Auto-Encoding Variational Bayes The International Conference on Learning\n",
        "Representations (ICLR), Banff, 2014. D.P. Kingma, M. Welling\n",
        "- Understanding the difficulty of training deep feedforward neural networks.\n",
        "X Glorot, Y Bengio. Aistats 9, 249-256\n",
        "- Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. \"Gradient-based\n",
        "learning applied to document recognition.\" Proceedings of the IEEE,\n",
        "86(11):2278-2324, November 1998.\n",
        "Links:\n",
        "- [VAE Paper] https://arxiv.org/abs/1312.6114\n",
        "- [Xavier Glorot Init](www.cs.cmu.edu/~bhiksha/courses/deeplearning/Fall.../AISTATS2010_Glorot.pdf).\n",
        "- [MNIST Dataset] http://yann.lecun.com/exdb/mnist/\n",
        "\n",
        "Author: Aymeric Damien\n",
        "Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import division, print_function, absolute_import\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "import tensorflow as tf\n",
        "\n",
        "# Import MNIST data\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.001\n",
        "num_steps = 30000\n",
        "batch_size = 64\n",
        "\n",
        "# Network Parameters\n",
        "image_dim = 784 # MNIST images are 28x28 pixels\n",
        "hidden_dim = 512\n",
        "latent_dim = 2\n",
        "\n",
        "# A custom initialization (see Xavier Glorot init)\n",
        "def glorot_init(shape):\n",
        "    return tf.random_normal(shape=shape, stddev=1. / tf.sqrt(shape[0] / 2.))\n",
        "\n",
        "# Variables\n",
        "weights = {\n",
        "    'encoder_h1': tf.Variable(glorot_init([image_dim, hidden_dim])),\n",
        "    'z_mean': tf.Variable(glorot_init([hidden_dim, latent_dim])),\n",
        "    'z_std': tf.Variable(glorot_init([hidden_dim, latent_dim])),\n",
        "    'decoder_h1': tf.Variable(glorot_init([latent_dim, hidden_dim])),\n",
        "    'decoder_out': tf.Variable(glorot_init([hidden_dim, image_dim]))\n",
        "}\n",
        "biases = {\n",
        "    'encoder_b1': tf.Variable(glorot_init([hidden_dim])),\n",
        "    'z_mean': tf.Variable(glorot_init([latent_dim])),\n",
        "    'z_std': tf.Variable(glorot_init([latent_dim])),\n",
        "    'decoder_b1': tf.Variable(glorot_init([hidden_dim])),\n",
        "    'decoder_out': tf.Variable(glorot_init([image_dim]))\n",
        "}\n",
        "\n",
        "# Building the encoder\n",
        "input_image = tf.placeholder(tf.float32, shape=[None, image_dim])\n",
        "encoder = tf.matmul(input_image, weights['encoder_h1']) + biases['encoder_b1']\n",
        "encoder = tf.nn.tanh(encoder)\n",
        "z_mean = tf.matmul(encoder, weights['z_mean']) + biases['z_mean']\n",
        "z_std = tf.matmul(encoder, weights['z_std']) + biases['z_std']\n",
        "\n",
        "# Sampler: Normal (gaussian) random distribution\n",
        "eps = tf.random_normal(tf.shape(z_std), dtype=tf.float32, mean=0., stddev=1.0,\n",
        "                       name='epsilon')\n",
        "z = z_mean + tf.exp(z_std / 2) * eps\n",
        "\n",
        "# Building the decoder (with scope to re-use these layers later)\n",
        "decoder = tf.matmul(z, weights['decoder_h1']) + biases['decoder_b1']\n",
        "decoder = tf.nn.tanh(decoder)\n",
        "decoder = tf.matmul(decoder, weights['decoder_out']) + biases['decoder_out']\n",
        "decoder = tf.nn.sigmoid(decoder)\n",
        "\n",
        "# Define VAE Loss\n",
        "def vae_loss(x_reconstructed, x_true):\n",
        "    # Reconstruction loss\n",
        "    encode_decode_loss = x_true * tf.log(1e-10 + x_reconstructed) \\\n",
        "                         + (1 - x_true) * tf.log(1e-10 + 1 - x_reconstructed)\n",
        "    encode_decode_loss = -tf.reduce_sum(encode_decode_loss, 1)\n",
        "    # KL Divergence loss\n",
        "    kl_div_loss = 1 + z_std - tf.square(z_mean) - tf.exp(z_std)\n",
        "    kl_div_loss = -0.5 * tf.reduce_sum(kl_div_loss, 1)\n",
        "    return tf.reduce_mean(encode_decode_loss + kl_div_loss)\n",
        "\n",
        "loss_op = vae_loss(decoder, input_image)\n",
        "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# Initialize the variables (i.e. assign their default value)\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Start training\n",
        "with tf.Session() as sess:\n",
        "    # Run the initializer\n",
        "    sess.run(init)\n",
        "\n",
        "    for i in range(1, num_steps+1):\n",
        "      # Prepare Data\n",
        "      # Get the next batch of MNIST data (only images are needed, not labels)\n",
        "      batch_x, _ = mnist.train.next_batch(batch_size)\n",
        "\n",
        "      # Train\n",
        "      feed_dict = {input_image: batch_x}\n",
        "      _, l = sess.run([train_op, loss_op], feed_dict=feed_dict)\n",
        "      if i % 1000 == 0 or i == 1:\n",
        "        print('Step %i, Loss: %f' % (i, l))\n",
        "\n",
        "    # Testing\n",
        "    # Generator takes noise as input\n",
        "    noise_input = tf.placeholder(tf.float32, shape=[None, latent_dim])\n",
        "    # Rebuild the decoder to create image from noise\n",
        "    decoder = tf.matmul(noise_input, weights['decoder_h1']) + biases['decoder_b1']\n",
        "    decoder = tf.nn.tanh(decoder)\n",
        "    decoder = tf.matmul(decoder, weights['decoder_out']) + biases['decoder_out']\n",
        "    decoder = tf.nn.sigmoid(decoder)\n",
        "\n",
        "    # Building a manifold of generated digits\n",
        "    n = 20\n",
        "    x_axis = np.linspace(-3, 3, n)\n",
        "    y_axis = np.linspace(-3, 3, n)\n",
        "\n",
        "    canvas = np.empty((28 * n, 28 * n))\n",
        "    for i, yi in enumerate(x_axis):\n",
        "      for j, xi in enumerate(y_axis):\n",
        "        z_mu = np.array([[xi, yi]] * batch_size)\n",
        "        x_mean = sess.run(decoder, feed_dict={noise_input: z_mu})\n",
        "        canvas[(n - i - 1) * 28:(n - i) * 28, j * 28:(j + 1) * 28] = \\\n",
        "        x_mean[0].reshape(28, 28)\n",
        "\n",
        "    plt.figure(figsize=(8, 10))\n",
        "    Xi, Yi = np.meshgrid(x_axis, y_axis)\n",
        "    plt.imshow(canvas, origin=\"upper\", cmap=\"gray\")\n",
        "    plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-1-66ed08d367a2>:30: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "Step 1, Loss: 644.553711\n",
            "Step 1000, Loss: 181.764313\n",
            "Step 2000, Loss: 177.786209\n",
            "Step 3000, Loss: 166.170380\n",
            "Step 4000, Loss: 167.514526\n",
            "Step 5000, Loss: 156.182404\n",
            "Step 6000, Loss: 154.927841\n",
            "Step 7000, Loss: 155.034912\n",
            "Step 8000, Loss: 151.085022\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}