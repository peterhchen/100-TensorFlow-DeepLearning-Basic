{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sparse-autoencoder.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "uPHSGhTB2Sf-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        },
        "outputId": "973eaca9-925e-483e-b2a6-6dbb4574f1c5"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import matplotlib.pyplot as plt\n",
        "# %matplotlib inline\n",
        "\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\")\n",
        "trX, trY, teX, teY = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels\n",
        "\n",
        "class SparseAutoEncoder(object):\n",
        " def __init__(self, m, n, eta = 0.01):\n",
        "   # m: Number of neurons in input/output layer\n",
        "   # n: number of neurons in hidden layer\n",
        "   self._m = m\n",
        "   self._n = n\n",
        "   self.learning_rate = eta\n",
        "\n",
        "   # Create the Computational graph\n",
        "   # Weights and biases\n",
        "   self._W1 = tf.Variable(tf.random_normal(shape=(self._m,self._n)))\n",
        "   self._W2 = tf.Variable(tf.random_normal(shape=(self._n,self._m)))\n",
        "   self._b1 = tf.Variable(np.zeros(self._n).astype(np.float32)) #bias for hidden layer\n",
        "   self._b2 = tf.Variable(np.zeros(self._m).astype(np.float32)) #bias for output layer\n",
        "\n",
        "   # Placeholder for inputs\n",
        "   self._X = tf.placeholder('float', [None, self._m])\n",
        "\n",
        "   self.y = self.encoder(self._X)\n",
        "   self.r = self.decoder(self.y)\n",
        "   error = self._X - self.r\n",
        "\n",
        "   self._loss = tf.reduce_mean(tf.pow(error, 2))\n",
        "   alpha = 7.5e-5\n",
        "   kl_div_loss = tf.reduce_sum(self.kl_div(0.02,   tf.reduce_mean(self.y,0)))\n",
        "   loss = self._loss + alpha * kl_div_loss\n",
        "   self._opt = tf.train.AdamOptimizer(self.learning_rate).minimize(loss)\n",
        "\n",
        " def encoder(self, x):\n",
        "   h = tf.matmul(x, self._W1) + self._b1\n",
        "   return tf.nn.sigmoid(h)\n",
        "\n",
        " def decoder(self, x):\n",
        "   h = tf.matmul(x, self._W2) + self._b2\n",
        "   return tf.nn.sigmoid(h)\n",
        "\n",
        " def set_session(self, session):\n",
        "   self.session = session\n",
        "\n",
        " def reduced_dimension(self, x):\n",
        "   h = self.encoder(x)\n",
        "   return self.session.run(h, feed_dict={self._X: x})\n",
        "\n",
        " def reconstruct(self,x):\n",
        "   h = self.encoder(x)\n",
        "   r = self.decoder(h)\n",
        "   return self.session.run(r, feed_dict={self._X: x})\n",
        "\n",
        " def kl_div(self, rho, rho_hat):\n",
        "   term2_num = tf.constant(1.)- rho\n",
        "   term2_den = tf.constant(1.) - rho_hat\n",
        "   kl = self.logfunc(rho,rho_hat) + self.logfunc(term2_num, term2_den)\n",
        "   return kl\n",
        "\n",
        " def logfunc(self, x1, x2):\n",
        "   return tf.multiply( x1, tf.log(tf.div(x1,x2)))\n",
        "\n",
        " def fit(self, X, epochs = 1, batch_size = 100):\n",
        "   N, D = X.shape\n",
        "   num_batches = N // batch_size\n",
        "\n",
        "   obj = []\n",
        "   for i in range(epochs):\n",
        "     #X = shuffle(X)\n",
        "     for j in range(num_batches):\n",
        "       batch = X[j * batch_size: (j * batch_size + batch_size)]\n",
        "       _, ob = self.session.run([self._opt,self._loss], feed_dict={self._X: batch})\n",
        "       if j % 100 == 0:\n",
        "         print('training epoch {0} batch {2} cost {1}'.format(i,ob, j))\n",
        "   obj.append(ob)\n",
        "   return obj\n",
        "\n",
        "Xtrain = trX.astype(np.float32)\n",
        "Xtest = teX.astype(np.float32)\n",
        "\n",
        "_, m = Xtrain.shape\n",
        "sae = SparseAutoEncoder(m, 256)\n",
        "\n",
        "#Initialize all variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  sae.set_session(sess)\n",
        "  err = sae.fit(Xtrain, epochs=5)\n",
        "  out = sae.reconstruct(Xtest[0:100])\n",
        "\n",
        "plt.plot(err)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('Reconstruction Loss (MSE)')\n",
        "\n",
        "# Plotting original and reconstructed images\n",
        "row, col = 2, 8\n",
        "idx = np.random.randint(0, 100, row * col // 2)\n",
        "f, axarr = plt.subplots(row, col, sharex=True, sharey=True, figsize=(20,4))\n",
        "\n",
        "for fig, row in zip([Xtest,out], axarr):\n",
        "  for i,ax in zip(idx,row):\n",
        "    ax.imshow(fig[i].reshape((28, 28)), cmap='Greys_r')\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-1-805be86db18b>:7: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "training epoch 0 batch 0 cost 0.4212132394313812\n",
            "training epoch 0 batch 100 cost 0.06033796817064285\n",
            "training epoch 0 batch 200 cost 0.04154346510767937\n",
            "training epoch 0 batch 300 cost 0.033563945442438126\n",
            "training epoch 0 batch 400 cost 0.026250679045915604\n",
            "training epoch 0 batch 500 cost 0.02546810917556286\n",
            "training epoch 1 batch 0 cost 0.021826304495334625\n",
            "training epoch 1 batch 100 cost 0.01945052482187748\n",
            "training epoch 1 batch 200 cost 0.01847517117857933\n",
            "training epoch 1 batch 300 cost 0.018404915928840637\n",
            "training epoch 1 batch 400 cost 0.015764115378260612\n",
            "training epoch 1 batch 500 cost 0.016518928110599518\n",
            "training epoch 2 batch 0 cost 0.014856508001685143\n",
            "training epoch 2 batch 100 cost 0.013202388770878315\n",
            "training epoch 2 batch 200 cost 0.013359985314309597\n",
            "training epoch 2 batch 300 cost 0.01387612335383892\n",
            "training epoch 2 batch 400 cost 0.012332911603152752\n",
            "training epoch 2 batch 500 cost 0.013077903538942337\n",
            "training epoch 3 batch 0 cost 0.011732625775039196\n",
            "training epoch 3 batch 100 cost 0.010410991497337818\n",
            "training epoch 3 batch 200 cost 0.010369052179157734\n",
            "training epoch 3 batch 300 cost 0.011216480284929276\n",
            "training epoch 3 batch 400 cost 0.010097833350300789\n",
            "training epoch 3 batch 500 cost 0.010687136091291904\n",
            "training epoch 4 batch 0 cost 0.009543315507471561\n",
            "training epoch 4 batch 100 cost 0.008942925371229649\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}